{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 pyspark编程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 本例题为朝乐门的《数据科学理论与实践》（清华大学出版社，2017）P217中的SparkR编程的Python语言版本。版权所有，转载请注明出处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （1）导入pyspark包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install pyspark\n",
    "# 从spark.sql模块中导入SparkSession\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （2）创建SparkSession会话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 实例化一个SparkSession，用于连接Spark集群\n",
    "# app名中不要带空格，否则会出错\n",
    "# 此处以本地模式加载集群\n",
    "spark = SparkSession.builder\\\n",
    "    .appName('My_App')\\\n",
    "    .master('local')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （3）读入数据，创建Spark数据框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读入数据，创建Spark弹性式分布数据集数据框（DataFrame）\n",
    "df = spark.read.csv('flights.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （4）显示DataFrame的模式信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看数据框结构\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （5）缓存DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: string, month: string, day: string, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: string, origin: string, dest: string, air_time: string, distance: string, hour: string, minute: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame对象的cache()方法，可缓存的数据框\n",
    "# 对应的存储级别默认为MEMORY_AND_DISK\n",
    "# DataFrame的缓存仅有默认级别\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （6）显示DataFrame的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|       1|       96|     235|       70|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
      "|2014|    1|  1|       4|       -6|     738|      -23|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
      "|2014|    1|  1|       8|       13|     548|       -4|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
      "|2014|    1|  1|      28|       -2|     800|      -23|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n",
      "|2014|    1|  1|      34|       44|     325|       43|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame对象的show方法用于查看数据框的内容\n",
    "# 查看前5条记录\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （7）显示DataFrame的列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'dep_time',\n",
       " 'dep_delay',\n",
       " 'arr_time',\n",
       " 'arr_delay',\n",
       " 'carrier',\n",
       " 'tailnum',\n",
       " 'flight',\n",
       " 'origin',\n",
       " 'dest',\n",
       " 'air_time',\n",
       " 'distance',\n",
       " 'hour',\n",
       " 'minute']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame对象的columns属性可查看数据框的列名\n",
    "# 但此属性不可修改\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （8）统计DataFrame的行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52535"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame对象的count方法用于统计数据框的行数\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （9）选择数据框的特定列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+---------+---------+\n",
      "|tailnum|flight|dest|arr_delay|dep_delay|\n",
      "+-------+------+----+---------+---------+\n",
      "| N508AS|   145| ANC|       70|       96|\n",
      "| N195UW|  1830| CLT|      -23|       -6|\n",
      "| N37422|  1609| IAH|       -4|       13|\n",
      "+-------+------+----+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame对象的select方法用于选择数据框特定的列\n",
    "spark_df_flights_selected = df.select(df['tailnum'], df['flight'],\n",
    "                                      df['dest'], df['arr_delay'],\n",
    "                                      df['dep_delay'])\n",
    "# 查看选择数据的前三条信息\n",
    "spark_df_flights_selected.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （10）注册DataFrame为临时视图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrame对象的createGlobalTempView方法可将数据框注册为临时视图对象\n",
    "# 该方法的参数即为临时视图的名称\n",
    "df.createTempView('flights_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （11）编写并执行SQL语句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|dest|arr_delay|\n",
      "+----+---------+\n",
      "| ANC|       70|\n",
      "| CLT|      -23|\n",
      "| IAH|       -4|\n",
      "+----+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 可使用SparkSession对象（本教程中为spark）的sql方法做SQl查询\n",
    "\n",
    "# 构造一个SQL语句\n",
    "sql_str = 'select dest, arr_delay from flights_view'\n",
    "\n",
    "# 执行SQL语句\n",
    "spark_destDF = spark.sql(sql_str)\n",
    "\n",
    "# 查看查询结果的内容\n",
    "spark_destDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （12）将Spark SQl结果写入硬盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o325.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:598)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalStateException: SparkContext has been shutdown\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2014)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\r\n\t... 45 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-66d035d83bab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 此处会新建一个Output_spark_destDF目录，\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 并在其下存储csv文件， 类似HDFS的存储\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mspark_destDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output_spark_destDF1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mH:\\Python\\Anaconda\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace)\u001b[0m\n\u001b[0;32m    764\u001b[0m                        \u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m                        ignoreTrailingWhiteSpace=ignoreTrailingWhiteSpace)\n\u001b[1;32m--> 766\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Python\\Anaconda\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Python\\Anaconda\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Python\\Anaconda\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o325.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:598)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalStateException: SparkContext has been shutdown\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2014)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\r\n\t... 45 more\r\n"
     ]
    }
   ],
   "source": [
    "# DataFrame对象的write.csv方法将数据框保存为csv文件\n",
    "# 此处会新建一个Output_spark_destDF目录，\n",
    "# 并在其下存储csv文件， 类似HDFS的存储\n",
    "spark_destDF.write.csv('Output_spark_destDF1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （13）读取已保存的Spark SQl语句结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/C:/Users/soloman/clm/Output_spark_destDF/part-00000-93c4c3fd-487d-4660-92e7-9f42363e6787-c000.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mH:\\Python\\Anaconda\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Python\\Anaconda\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o53.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:/Users/soloman/clm/Output_spark_destDF/part-00000-93c4c3fd-487d-4660-92e7-9f42363e6787-c000.csv;\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:360)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:348)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:348)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:533)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3eb0934f4607>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# SparkSession对象的read.csv方法将csv文件读取为弹性式分布的DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdfnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output_spark_destDF/part-00000-93c4c3fd-487d-4660-92e7-9f42363e6787-c000.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 查看DataFrame对象的内容\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdfnew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Python\\Anaconda\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine)\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Python\\Anaconda\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Python\\Anaconda\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Path does not exist: file:/C:/Users/soloman/clm/Output_spark_destDF/part-00000-93c4c3fd-487d-4660-92e7-9f42363e6787-c000.csv;'"
     ]
    }
   ],
   "source": [
    "# SparkSession对象的read.csv方法将csv文件读取为弹性式分布的DataFrame\n",
    "dfnew = spark.read.csv('Output_spark_destDF/part-00000-93c4c3fd-487d-4660-92e7-9f42363e6787-c000.csv')\n",
    "\n",
    "# 查看DataFrame对象的内容\n",
    "dfnew.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （14）过滤DataFrame的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|     654|       -6|    1455|      -10|     DL| N686DA|   418|   SEA| JFK|     273|    2422|   6|    54|\n",
      "|2014|    1|  1|     708|       -7|    1510|      -19|     AA| N3DNAA|   236|   SEA| JFK|     281|    2422|   7|     8|\n",
      "|2014|    1|  1|     708|       -2|    1453|      -20|     DL| N3772H|  2258|   PDX| JFK|     267|    2454|   7|     8|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame对象的filter方法用于筛选数据框中的行\n",
    "jfkDF = df.filter(df['dest'] == 'JFK')\n",
    "jfkDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （15）分组统计Spark数据框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------+\n",
      "|day|      avg(arr_delay)|    avg(dep_delay)|\n",
      "+---+--------------------+------------------+\n",
      "|  7|0.025215252152521524| 5.243243243243243|\n",
      "| 15|  1.0819155639571518| 4.818353236957888|\n",
      "| 11|   5.749170537491706| 7.250661375661376|\n",
      "| 29|   6.407451923076923| 11.32174955062912|\n",
      "|  3|   5.629350893697084|11.526241799437676|\n",
      "| 30|   9.433526011560694| 12.31663788140472|\n",
      "|  8|    0.52455919395466| 4.555904522613066|\n",
      "| 22| -1.0817571690054912|  6.10231425091352|\n",
      "| 28| -3.4050632911392404| 4.110270951480781|\n",
      "| 16| 0.31582125603864736|4.2917420132610005|\n",
      "|  5|    4.42015503875969| 8.219989696032973|\n",
      "| 31|   5.796638655462185| 6.382229673093042|\n",
      "| 18|  -0.235370611183355|3.0194931773879143|\n",
      "| 27|  -4.354777070063694| 4.864126984126984|\n",
      "| 17|  1.8664688427299703| 5.873815165876778|\n",
      "| 26| -1.5248683440608544| 4.833430742255991|\n",
      "|  6|  3.1785932721712538| 7.075045759609518|\n",
      "| 19|  2.8462462462462463| 7.208383233532934|\n",
      "| 23|   2.352836879432624| 6.307105108631826|\n",
      "| 25| -2.3858004018754184|3.4145527369826434|\n",
      "+---+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- day: string (nullable = true)\n",
      " |-- avg(arr_delay): double (nullable = true)\n",
      " |-- avg(dep_delay): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用DataFrame对象的groupBy方法实现分组统计，agg方法实现聚合\n",
    "# groupBy方法接受一个列为参数，作为分组依据\n",
    "# agg方法接受一个字典作为参数，一个键值对对应一个列的操作\n",
    "# 键（key）表示待聚合的列的类名\n",
    "# 值（value）表示聚合使用的方法\n",
    "dailyDelayDF = df.groupBy(df.day)\\\n",
    "                 .agg({'dep_delay': 'mean', 'arr_delay':'mean'})\n",
    "\n",
    "# 使用DataFrame对象的show方法显示数据框的内容,\n",
    "# 从显示结果可以看出，计算结果为“所有航班的每日平均延误起飞时间和每日平均延误降落\n",
    "dailyDelayDF.show()\n",
    "\n",
    "# 查看数据框模式信息\n",
    "dailyDelayDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （16）重命名DataFrame数据框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- day: string (nullable = true)\n",
      " |-- avg_arr_delay: double (nullable = true)\n",
      " |-- avg_dep_delay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame对象的withwithColumnRenamed方法可实现更名\n",
    "# 接受两个参数，分别为就列名和新列名\n",
    "# 但需要注意的是该方法并不会直接在原数据框上进行操作，\n",
    "# 而是返回一个更名后新的数据框\n",
    "dailyDelayDF = dailyDelayDF.withColumnRenamed('avg(arr_delay)', 'avg_arr_delay')\n",
    "dailyDelayDF = dailyDelayDF.withColumnRenamed('avg(dep_delay)', 'avg_dep_delay')\n",
    "dailyDelayDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （17）将数据转换为本地数据框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>avg_arr_delay</th>\n",
       "      <th>avg_dep_delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0.025215</td>\n",
       "      <td>5.243243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>1.081916</td>\n",
       "      <td>4.818353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>5.749171</td>\n",
       "      <td>7.250661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>6.407452</td>\n",
       "      <td>11.321750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>5.629351</td>\n",
       "      <td>11.526242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>9.433526</td>\n",
       "      <td>12.316638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.524559</td>\n",
       "      <td>4.555905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>-1.081757</td>\n",
       "      <td>6.102314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28</td>\n",
       "      <td>-3.405063</td>\n",
       "      <td>4.110271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>0.315821</td>\n",
       "      <td>4.291742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  day  avg_arr_delay  avg_dep_delay\n",
       "0   7       0.025215       5.243243\n",
       "1  15       1.081916       4.818353\n",
       "2  11       5.749171       7.250661\n",
       "3  29       6.407452      11.321750\n",
       "4   3       5.629351      11.526242\n",
       "5  30       9.433526      12.316638\n",
       "6   8       0.524559       4.555905\n",
       "7  22      -1.081757       6.102314\n",
       "8  28      -3.405063       4.110271\n",
       "9  16       0.315821       4.291742"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame对象的toPandas方法可将弹性式分布数据框装换为本地的pandas数据框\n",
    "local_dailyDelay = dailyDelayDF.toPandas()\n",
    "\n",
    "# 查看pandas数据框前10行内容\n",
    "local_dailyDelay.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （18）结果的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x17ad043b080>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEdRJREFUeJzt3X+sX/Vdx/HXi7bES928EO4WepmWmaUxoQtdbhStQTLG\nikqkNtNAsoTpTP3DH+BipWgizGS22mlmYgKpDoUMwYV1d0QyCllH0GTgbrmM4lhlKmP9Fuld8Iro\nVUp5+8c9l5bb+733fO/3+z2fc87n+Uiae++5p/f7OT3t99Xz/rzP+TgiBADI1zmpBwAASIsgAIDM\nEQQAkDmCAAAyRxAAQOYIAgDIHEEAAJkjCAAgcwQBAGRubeoBlHHhhRfGxo0bUw8DABrl8OHD34uI\nsZX2a0QQbNy4UVNTU6mHAQCNYvs7ZfajNAQAmSMIACBzBAEAZI4gAIDMEQQAkDmCAAAy14j2UQBo\ng8npjvYdPKrjs3PaMDqiXds2afuW8dTDIggAoAqT0x3deuCI5k6ekiR1Zud064EjkpQ8DCgNAUAF\n9h08+lYILJg7eUr7Dh5NNKLTCAIAqMDx2bmetleJIACACmwYHelpe5UIAgCowK5tmzSybs3bto2s\nW6Nd2zYlGtFpQwsC23fZPmH72TO27bP9LdvP2P6i7dFhvT4A1Mn2LePas2OzxkdHZEnjoyPas2Nz\n8oliSXJEDOcH21dIek3SPRFxabHtw5IORcQbtv9IkiLilpV+1sTERPD0UQDoje3DETGx0n5DuyKI\niMclvbJo2yMR8Ubx5ROSLh7W6wMAykk5R/DLkr7c7Zu2d9qesj01MzNT4bAAIC9JgsD270l6Q9K9\n3faJiP0RMRERE2NjKy6wAwBYpcrvLLZ9o6RrJV0Vw5qgAACUVmkQ2L5G0i2Sfioi/qfK1wYALG2Y\n7aP3SfqapE22j9n+uKQ/l/QOSY/aftr2ncN6fQBAOUO7IoiIG5bY/NlhvR4AYHW4sxgAMsdjqHtQ\n12eJA0A/CIKS6vwscQDoB6Whkur8LHEA6AdBUFKdnyUOAP0gCEqq87PEAaAfBEFJdX6WOIC0Jqc7\n2rr3kC7Z/ZC27j2kyelO6iH1hMnikhYmhOkaAnCmNjSSEAQ92L5lvDEnFkA1lmskacr7BaUhAOhD\nGxpJCAIA6EMbGkkIAgDoQxsaSZgjAIA+tKGRhCAAgD41vZGE0hAAZI4gAIDMEQQAkDmCAAAyRxAA\nQOYIAgDIHEEAAJkjCAAgcwQBAGSOIACAzBEEAJA5ggAAMkcQAEDmCAIAyBxBAACZIwgAIHMEAQBk\njiAAgMwRBACQuaEFge27bJ+w/ewZ2y6w/ajt54uP5w/r9QEA5QzziuCvJV2zaNtuSV+JiPdJ+krx\nNQAgoaEFQUQ8LumVRZuvk3R38fndkrYP6/UBAOVUPUfw7oh4SZKKj++q+PUBAIvUdrLY9k7bU7an\nZmZmUg8HAFqr6iB42fZFklR8PNFtx4jYHxETETExNjZW2QABIDdVB8GDkm4sPr9R0pcqfn0AwCLD\nbB+9T9LXJG2yfcz2xyXtlXS17eclXV18DQBIaO2wfnBE3NDlW1cN6zUBAL0bWhDkbHK6o30Hj+r4\n7Jw2jI5o17ZN2r5lPPWwAGBJBMGATU53dOuBI5o7eUqS1Jmd060HjkgSYQCglmrbPtpU+w4efSsE\nFsydPKV9B48mGhEALI8gGLDjs3M9bQeA1AiCAdswOtLTdgBIjSAYsF3bNmlk3Zq3bRtZt0a7tm1K\nNCIAWB6TxQO2MCFM1xCApiAIhmD7lnHe+AE0BqUhAMgcQQAAmSMIACBzBAEAZI4gAIDMEQQAkDmC\nAAAyRxAAQOYIAgDIHEEAAJkjCAAgcwQBAGSOIACAzGX/9FEWmgeQu6yDgIXmASDz0hALzQNA5kHA\nQvMAkHlpaMPoiDpLvOmz0HzvmGsBmivrKwIWmh+MhbmWzuycQqfnWianO6mHlrXJ6Y627j2kS3Y/\npK17D3E+0FXWQbB9y7j27Nis8dERWdL46Ij27NjM/2R7xFxL/RDO6EWp0pDt319hlxMRcecAxlM5\nFprvH3Mt9bNcOPP3HYuVnSO4XNL1ktzl+3dLamQQpNSWujpzLfVDOKMXZUtDpyLi1Yj4z6V+SYph\nDrKN2nTpzlxL/XQLYcIZSykbBCu90RMEPWpTXZ25lvohnNGLsqWhdbbf2eV7lrSmy/fQRdsu3Qc9\n19KWslkqC39W/BmijLJB8ISkm5f5/pd7eVHbvyXpVzR/JXFE0i9FxP/28jOajrp6dzz6YzBohEBZ\nvbSPeplf5X+IPS7pNyVNRMSlmr+auL6Xn9EGXLp316ayGdAEZa8IfkyD7RpaK2nE9klJ50k63sPv\nbQUu3btrW9kMqLuyQXAqIl7t9k3bpSeLI6Jj+9OSXpQ0J+mRiHik7O9vEy7dl0bZDKhW5V1Dts+X\ndJ2kSyRtkLTe9keX2G+n7SnbUzMzM2V/PFqAshlQrbJBsM72O7v8+gH11jX0IUn/FhEzEXFS0gFJ\nP7F4p4jYHxETETExNjbWw49H09GOClQrRdfQi5Iut32e5ktDV0ma6uH3IwOUzYDqVN41FBFPSnpA\n0lOabx09R9L+Xn4GAGBwknQNRcRtkm4ruz8AYHgq7xoCANQLzxoCgMzxrCEAyFySZw0BAOqjl8Xr\ne+oOAgA0Q6pnDQEAaoKuIQDIHF1DAJA5uoZQKVYeA+qHriFUhpXHBoMwxaBV/qwh5IuVx/q3EKad\n2TmFTofp5HQn9dDQYHQNoTKsPNa/5cKUqwKsFl1DDdCWUgArj/WPMMUw0DVUc20qBbDyWP+6hSZh\nin6kWKEMPWhTXZ2Vx/pHmGIYBtE1ZNE1NDRtKwWw8lh/Fv7s2lAqRH0wWVxz1NWxGGGKQWvtZHFb\nJlh3bdv0tt57iVIAgMEqGwSNmixu041LlAIADFsrHzHRtl5rSgEAhqnXyeJucwQPD2Y4g9G2CVZg\nmNpSRsXqlQqCiPjksAcySEywAuW0qYyK1evlWUONQa81UE6b7lPB6vWyVGVjMMEKlEMZFVJLg0Bi\nghUogzIqpJaWhgCUQxkVUouvCACsjDJqPVXdyUUQAJmjjFovKTq5CAIAA8V9Cf1JcUMsQQBgYLgv\noX8pOrmYLAYwMNyX0L8Uiw8RBAAGhvsS+peik4vSEIBSytT+uS+hfyk6uQgCACsqW/tn/YzBqLqT\nK0lpyPao7Qdsf8v2c7Z/PMU4AJRTtvbPutTNlOqK4M8kPRwRH7F9rqTzEo0DQAm91P65L6F5Kg+C\nYoGbKyR9TJIi4nVJr1c9DqyMfnAsoPbfbilKQ++VNCPpr2xP2/5L2+sX72R7p+0p21MzMzPVjzJz\nCzXhzuycQqdrwpPTndRDa5TJ6Y627j2kS3Y/pK17DzX2z49nErVbiiBYK+kDku6IiC2S/lvS7sU7\nRcT+iJiIiImxsbGqx5g9+sH716YwpfbfbinmCI5JOhYRTxZfP6AlggBp0Q/eP9bORlNUfkUQEf8u\n6bu2F64pr5L0zarHgeWluLuxbQhTNEWqO4t/Q9K9tp+RdJmkP0w0DnTRlJpwnWvwhCmaIkkQRMTT\nRf3//RGxPSL+I8U40F0TasJ1r8E3JUwB7ixGV3WvCde9Bs+iL2gKggCN1YQafN3DFJB4+igajBo8\nMBgEARqLGjwwGJSG0Fi91OB5XAbQHUGARitTg2f5RGB5lIbQejwuA1geVwQtQvljaU3oLgJS4oqg\nJep+c1VKdBcByyMIWoLyR3d0FwHLozTUEpQ/uuMOX2B5BEFLsILU8rjDF+iO0lBLUP4AsFpcEbQE\n5Q8Aq0UQtAjlDwCrQWkIADLHFQFwBm7Ka7ay54/z/HYEAVDgmUTNVvb8cZ7PRmkIKHBTXrOVPX+c\n57MRBECBm/Karez54zyfjSAACjyTqNnKnj/O89kIAqDATXnNVvb8cZ7PxmRxhuiYWBo35TVb2fPH\neT6bIyL1GFY0MTERU1NTqYfRCos7JqT5/w3t2bE5638IQBvZPhwREyvtR2koM3RMAFiMIMgMHRMA\nFiMIMkPHBIDFCILM0DEBYDG6hjJDx8Rg0HmFNiEIMsTjqvvDs2rQNpSGgB7ReYW2IQiAHtF5hbah\nNAT0aMPoiDpLvOnTedU+ucwFJbsisL3G9rTtv0s1BmA16LzKw8JcUGd2TqHTc0GT053UQxu4lKWh\nmyQ9l/D1gVXZvmVce3Zs1vjoiCxpfHSER3S0UE5zQUlKQ7YvlvSzkj4l6RMpxgD0g86r9stpLijV\nFcFnJP2OpDcTvT4ALCunu/ArDwLb10o6ERGHV9hvp+0p21MzMzMVjQ4A5uU0F5TiimCrpJ+z/YKk\n+yV90PbnFu8UEfsjYiIiJsbGxqoeI4DM5TQXlHQ9AttXSvrtiLh2uf1YjwAAeld2PQLuIwCQRC49\n+k2QNAgi4jFJj6UcA4Dq8bymeuEREwAql1OPfhMQBAAql1OPfhMQBAAql1OPfhMQBAAql1OPfhPQ\nNQSgcr2ulEeH0XARBACSKPu8JjqMho/SEIBao8No+AgCALVGh9HwEQQAao0Oo+EjCADUGh1Gw8dk\nMYBa67XDCL0jCADUHivCDRelIQDIHFcEQE1w0xRSIQiAGuCmKaREaQioAW6aQkoEAVAD3DSFlAgC\noAa4aQopEQRADXDTFFJishioAW6aQkoEAVAT3DSFVCgNAUDmCAIAyBxBAACZIwgAIHMEAQBkzhGR\negwrsj0j6TuLNl8o6XsJhjMMHEv9tOU4JI6ljqo6jh+KiLGVdmpEECzF9lRETKQexyBwLPXTluOQ\nOJY6qttxUBoCgMwRBACQuSYHwf7UAxggjqV+2nIcEsdSR7U6jsbOEQAABqPJVwQAgAFoZBDYvsb2\nUdvftr079XhWy/YLto/Yftr2VOrx9ML2XbZP2H72jG0X2H7U9vPFx/NTjrGsLsdyu+1OcW6etv0z\nKcdYhu332P6q7eds/5Ptm4rtjTsvyxxLE8/L99n+R9vfKI7lk8X2S2w/WZyXv7V9brIxNq00ZHuN\npH+WdLWkY5K+LumGiPhm0oGtgu0XJE1EROP6om1fIek1SfdExKXFtj+W9EpE7C0C+vyIuCXlOMvo\nciy3S3otIj6dcmy9sH2RpIsi4inb75B0WNJ2SR9Tw87LMsfyi2reebGk9RHxmu11kv5B0k2SPiHp\nQETcb/tOSd+IiDtSjLGJVwQ/KunbEfGvEfG6pPslXZd4TNmJiMclvbJo83WS7i4+v1vz/3Brr8ux\nNE5EvBQRTxWf/5ek5ySNq4HnZZljaZyY91rx5briV0j6oKQHiu1Jz0sTg2Bc0nfP+PqYGvoXRPN/\nGR6xfdj2ztSDGYB3R8RL0vw/ZEnvSjyefv267WeK0lHtyylnsr1R0hZJT6rh52XRsUgNPC+219h+\nWtIJSY9K+hdJsxHxRrFL0vexJgaBl9jWrPrWaVsj4gOSflrSrxUlCtTDHZJ+WNJlkl6S9Cdph1Oe\n7e+X9AVJN0fEq6nH048ljqWR5yUiTkXEZZIu1nxV40eW2q3aUZ3WxCA4Juk9Z3x9saTjicbSl4g4\nXnw8IemLmv8L0mQvF7XdhRrvicTjWbWIeLn4x/umpL9QQ85NUYP+gqR7I+JAsbmR52WpY2nqeVkQ\nEbOSHpN0uaRR2wurRCZ9H2tiEHxd0vuKGfdzJV0v6cHEY+qZ7fXFJJhsr5f0YUnPLv+7au9BSTcW\nn98o6UsJx9KXhTfOws+rAeemmJT8rKTnIuJPz/hW485Lt2Np6HkZsz1afD4i6UOan/P4qqSPFLsl\nPS+N6xqSpKJl7DOS1ki6KyI+lXhIPbP9Xs1fBUjza0f/TZOOw/Z9kq7U/FMUX5Z0m6RJSZ+X9IOS\nXpT0CxFR+0nYLsdypebLDyHpBUm/ulBnryvbPynp7yUdkfRmsfl3NV9bb9R5WeZYblDzzsv7NT8Z\nvEbz//n+fET8QfEecL+kCyRNS/poRPxfkjE2MQgAAIPTxNIQAGCACAIAyBxBAACZIwgAIHMEAQBk\njiAAgMwRBACQubUr7wJAeuvR1JdLWnhQ2FpJTyy1LSJur3p8wGoRBEBvri+eF6PisQE3d9kGNAal\nIQDIHEEAAJkjCAAgcwQBAGSOIACAzBEEAJA52keB8k5Iusf2wkIp50h6uMs2oDFYmAYAMkdpCAAy\nRxAAQOYIAgDIHEEAAJkjCAAgc/8PYSR+k3SNl7gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17acfc0cfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 设置matplotlib绘图为行内显示，可在jupyter notebook中直接绘出图像\n",
    "# 否则需调用show方法显示绘制的图像\n",
    "%matplotlib inline\n",
    "\n",
    "# 导入matplotlib包的pyplot模块，取别名为plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 绘制“日期-起飞”散点图\n",
    "# 此处将点的坐标转换为数值类型，详见numpy数组的操作\n",
    "plt.scatter(local_dailyDelay.day.values.astype('i8'),\n",
    "            local_dailyDelay.avg_dep_delay.astype('f8'))\n",
    "\n",
    "# 设置轴名\n",
    "plt.xlabel('日期')\n",
    "plt.ylabel('起飞延误时间')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x17ad061e978>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFR5JREFUeJzt3X2QXXddx/H31ySVLVpSaCB005pWJQotNs6CQJjC9IFE\nLTZ0FMpQp/VhojM+gA+RVmYEnHEarSLM6MBERWCIlA6E8NxCqVAVqN2Q1D4ZwFJpNg3ZFtNSmoGk\nfP1jzzbZ53t3772/e+55v2Yy2XvO3bvfM+fufu75PZ3ITCRJ+qHSBUiS+oOBIEkCDARJUsVAkCQB\nBoIkqWIgSJIAA0GSVDEQJElADwIhIt4dEYci4q4Ttj09Ij4bEV+r/j+123VIkuYX3Z6pHBHnA48B\n78vMc6ptfwV8OzO3RcTVwKmZ+caFXuu0007LtWvXdrVeSRo0u3fvfigzVy30vOXdLiQzb42ItdM2\nXwq8vPr6vcDngQUDYe3atYyOjnawOkkafBHxv608r1QfwrMy80GA6v9nzvXEiNgSEaMRMTo+Pt6z\nAiWpafq+Uzkzt2fmSGaOrFq14BWPJGmRSgXCtyLi2QDV/4cK1SFJqpQKhI8BV1ZfXwl8tFAdkqRK\nL4adfgD4ErAuIvZHxG8A24CLI+JrwMXVY0lSQb0YZfTaOXZd2O2fLUn9aNeeMa67aR8HDh/h9JVD\nbN24js3rh0uX1f1AkCQdt2vPGNfsvJMjR58AYOzwEa7ZeSdA8VDo+1FGkjRIrrtp35NhMOnI0Se4\n7qZ9hSo6zkCQpB46cPhIW9t7yUCQpB46feVQW9t7yUCQpB7aunEdQyuWTdk2tGIZWzeuK1TRcXYq\nS1IPTXYcO8pIksTm9cN9EQDT2WQkSQIMBElSxUCQJAEGgiSpYiBIkgADQZJUMRAkSYCBIEmqGAiS\nJMBAkCRVDARJEmAgSJIqRQMhIv4gIu6OiLsi4gMR8ZSS9UhSkxULhIgYBn4fGMnMc4BlwOWl6pGk\npivdZLQcGIqI5cDJwIHC9UhSYxULhMwcA/4a+CbwIPBIZn6mVD2S1HQlm4xOBS4FzgJOB54aEVfM\n8rwtETEaEaPj4+O9LlOSGqNkk9FFwDcyczwzjwI7gZdMf1Jmbs/MkcwcWbVqVc+LlKSmKBkI3wRe\nFBEnR0QAFwL3FqxHkhqtZB/CbcCHgK8Ad1a1bC9VjyQ13fKSPzwz3wy8uWQNkqQJpYedSpL6hIEg\nSQIMBElSxUCQJAEGgiSpYiBIkgADQZJUMRAkSYCBIEmqGAiSJMBAkCRVDARJEmAgSJIqBoIkCTAQ\nJEmVovdDkKRBsmvPGNfdtI8Dh49w+sohtm5cx+b1w6XLapmBIEkdsGvPGNfsvJMjR58AYOzwEa7Z\neSdAbULBJiNJ6oDrbtr3ZBhMOnL0Ca67aV+hitpnIEhSBxw4fKSt7f3IQJCkDjh95VBb2/tR0UCI\niJUR8aGI+O+IuDciXlyyHklarK0b1zG0YtmUbUMrlrF147pCFbWvdKfyO4AbM/OXI+Ik4OTC9UjS\nokx2HDvKaBEi4hTgfOAqgMz8PvD9UvVI0lJtXj9cqwCYruQVwtnAOPDPEfEzwG7g9Zn53YI1SVLf\n6PW8hpJ9CMuBnwXemZnrge8CV09/UkRsiYjRiBgdHx/vdY3qkl17xtiw7RbOuvqTbNh2C7v2jJUu\nSeork/Maxg4fITk+r6GbvyslA2E/sD8zb6sef4iJgJgiM7dn5khmjqxataqnBao7SrzRpbopMa+h\nWCBk5kHggYiY7IK/ELinVD3t8NPt0gzCBB6p20rMayg9yuj3gB3VCKP7gF8rXM+CBmF6emmDMIFH\n6rbTVw4xNsvvRDfnNRSdh5CZe6vmoOdn5ubM/L+S9bTCT7fza+XqaRAm8EjdVmJegzOV2+Sn27m1\n2jcwCBN4pG7bvH6Yay87l+GVQwQwvHKIay87t6stEaWbjGqnxGVcXcx39XTim3gQJvBIvdDreQ0G\nQpu2blw3pQ8B/HQ7qZ2rp7pP4JEGkU1GbSpxGVcX9g1I9eYVwiL46XZ2Xj1J9WYgqGPsG5DqzUBQ\nR3n1JNWXfQiSJMBAkCRVDARJEmAfwpN6ve64JPUbAwEXrJMksMkIcME6SQIDAXDBOkkCm4yA8gvW\n2X+hbvL9pVZ5hUDZ5Zi9naS6yfeX2mEgUHbBOvsv1E2+v9QOm4wqpZZcsP9C3eT7S+3wCqEwl4xW\nN/n+UjsMhMK8naS6yfeX2lG8ySgilgGjwFhmXlK6nl5zyWh1k+8vtSMys2wBEX8IjACnLBQIIyMj\nOTo62pvCJGlARMTuzBxZ6HlFm4wiYg3wi8A/lqxDklS+D+HtwJ8AP5jrCRGxJSJGI2J0fHy8d5VJ\nUsMUC4SIuAQ4lJm753teZm7PzJHMHFm1alWPqpOk5il5hbAB+KWIuB+4HrggIt5fsB5JarRio4wy\n8xrgGoCIeDnwx5l5Ral61Ayu6yPNrfiwU6lXvO+FNL+WAiEi/myBpxzKzHcttojM/Dzw+cV+vwZX\nJz/Rz7euj4EgtX6F8CLgciDm2P9eYNGBIM2m05/oXddHml+rncpPZOajmfnIbP+AsrPbNJA6vVKn\n6/pI82s1EBb6g28gqOM6/YnedX2k+bXaZLQiIk6ZY18Ay+bY12idHtHStBEynb6Tnev6SPNrNRC+\nDLxhnv2f7kAtA6XT7d9NHCGzdeO6KccMS/9EX+q+F1IdtDMxLeb5p2k63f7dxDtflbyTndRErV4h\n/ByOMmpLp9u/mzpCxk/0Uu+0GghPZOajc+2MCDuVp+l0+3enX0+SpnOUUZd0ekSLI2Q03a49Y2zY\ndgtnXf1JNmy7hV17xkqXpJpzlFGXdHpEiyNkdKImDjJQ97V0x7SIeDPzXwUsaemKVnnHNGnChm23\nzNqEOLxyiP+4+oICFamftXrHtHYWt3M0kdQnmjrIQN3lKCOphhxkoG5wLSOphhxkoG5o9QrBUUZS\nH3GQgbrBUUZSTfX7pL2mrb01CFzLSFLHOSy2nlzLSFLHNXHtrUEw8KOMvGyVes9hsfVUbC2jiDgD\neB+wGvgBsD0z39Hu68zHy1apDIfF1lPJtYyOAX+UmT/NxD2bfycinruI15mTl61SGQ6Lradio4wy\n80Hgwerr70TEvcAwcE+7rzUXL1ulMhwWW099McooItYC64HblvI603nZqn7RxL6sfh8Wq5mKjzKK\niB8BPgy8YbZ+iojYEhGjETE6Pj7e1mt72ap+MNmXNXb4CMnxviyXq1a/KTrKKCJWMBEGOzJz52zP\nycztwHaYWO20ndf3slX9YL6+LN+L6iclRxkF8E/AvZn5tna/v1Vetqo0+7JUFyVHGW0AfhW4ICL2\nVv9+YRGvI/W1ufqs7MtSvyk5yujfcZazGmDrxnVT5sOAfVnqT50YZRS4lpEK6+dRPPZlqS4GfukK\nDb46zEi3L0t14A1yVHvOSJc6o2SnstQRjuKROsMb5GhB/dw+D85Ilzql3U7lufoQbuxMOeo3dWif\n78Yonn4PQakbWgqEzHxrtwtRf6rDLNtOj+KpQwhK3dDqFYIaqi7t850cxVOHEJS6oZ3F7dRATZxl\nW5cQlDrNQNC8mrhibBNDUAIDQQvYvH6Yay87l+GVQwQwvHKIay87d6CbTpoYghLYh6AWNG2WrUtN\nqKkMBGkWTQtBCQyERnOsvaQTGQgN5Vh7SdPZqdxQLggnaTqvEBrKsfadYbObBolXCA3lWPulm2x2\nGzt8hOR4s9uuPWOlS5MWxUBoKMfaL53Nbho0Nhk1lGPtl85mNw2aooEQEZuAdzBxP4V/zMxtJesZ\nFK22azvWfmm8D4MGTbEmo4hYBvw98PPAc4HXRsRzS9UzKGzX7h2b3TRoSl4hvBD4embeBxAR1wOX\nAvfM9Q0PP/ww73nPe6Zse97znscLXvACjh49yo4dO2Z8z3nnncd5553H448/zg033DBj/8jICOec\ncw6PPPIIH/nIR2bsf/GLX8y6det46KGH+MQnPjFj//nnn8/ZZ5/NwYMHufHGmfcJuvDCCznjjDN4\n4IEH+NznPjdj/6ZNm1i9ejX33Xcft95664z9l1xyCaeddhr79u3jS1/60oz9r3rVq3ja057GXXfd\nxejoKH/xyXv4v8ePPrn/5J96KUd4Cm/5509y+I6TZnz/6173OlasWMHtt9/O3XffPWP/VVddBcAX\nv/hFvvrVr07Zt3z5cq644goAvvCFL/CNb3xjyv6hoSFe85rXAHDzzTezf//+KftPOeUULrvsMgBu\nvPFGDh48OGX/M57xDF75ylcC8PGPf5yHH354yv7Vq1ezadMmAHbu3Mmjjz46Zf+aNWu46KKLAPjg\nBz/IkSNTP82fddZZvOxlLwPg/e9/P8eOHZuy/znPeQ4veclLAGa872DivXftZefyl5+6m6998dOc\nevIKLj7n2Ry+4yDvuaN5773pXv3qV3PyySezd+9e9u7dO2O/772lvfeW8ndvLiUDYRh44ITH+4Gf\nm/6kiNgCbIGJk6T5nRgGJzr0ne8BMwNBS7N5/TC/eM4z2bHjQOlSpCWLzCzzgyN+BdiYmb9ZPf5V\n4IWZ+Xtzfc/IyEjO9klEx23Ydsus7drDK4f4j6svKFCR2tW0uQ1NO94SImJ3Zo4s9LySw073A2ec\n8HgN4MesJbJdu96a1gfUtOPtdyUD4XbgJyPirIg4Cbgc+FjBegZCE+9fMEiaNrehacfb74r1IWTm\nsYj4XeAmJoadvjszZ/YsqW0OJ62vps1taNrx9rui8xAy81PAp0rWIPWTps1taNrx9juXrpD6SNP6\ngJp2vP3OpSukPtK0JUWadrz9rtiw08Vw2Kkkta8Ow04lSX3EJiNJgBPEZCBIwntsa4JNRpKcICbA\nKwRJtDdBzKalweUVgqSW77Ht2kODzUCQ1PIEMZuWBptNRpJaniDm2kODzUCQBLS2KKJrDw02m4wk\ntcy1hwabVwiSWlaXtYccCbU4BoKktvT7/TacZLd4NhlJGiiOhFo8A0HSQHEk1OIZCJIGSquT7DST\ngSBpoDgSavGKdCpHxHXAK4HvA/8D/FpmHi5Ri6TBUpeRUP2oyB3TIuIVwC2ZeSwi/hIgM9+40Pd5\nxzRJal9f3zEtMz+Tmceqh18G1pSoQ5J0XD/0Ifw68OnSRUhS03WtDyEibgZWz7LrTZn50eo5bwKO\nATvmeZ0twBaAM888swuVSpKgi4GQmRfNtz8irgQuAS7MeToyMnM7sB0m+hA6WqQk6UmlRhltAt4I\nvCwzHy9RgyRpqlJrGf0d8MPAZyMC4MuZ+duFapFUEy5a111FAiEzf6LEz5VUXy5a1339MMpIkhbk\nonXdZyBIqgUXres+A0FSLbhoXfcZCJJqwUXrus87pkmqBRet6z4DQVJt9PvtO+vOJiNJEmAgSJIq\nBoIkCTAQJEkVA0GSBBgIkqSKw04laQFNWWXVQJCkeTRplVWbjCRpHk1aZdVAkKR5NGmVVQNBkubR\npFVWDQRJmkeTVlm1U1mS5tGkVVYNBElaQFNWWS3aZBQRfxwRGRGnlaxDklQwECLiDOBi4JulapAk\nHVfyCuFvgT8BsmANkqRKkUCIiF8CxjLzjhI/X5I0U9c6lSPiZmD1LLveBPwp8IoWX2cLsAXgzDPP\n7Fh9kqSpIrO3LTYRcS7wOeDxatMa4ADwwsw8ON/3joyM5OjoaJcrlKTBEhG7M3Nkoef1fNhpZt4J\nPHPycUTcD4xk5kO9rkVSszVlFdNWOQ9BUiM1aRXTVhVfuiIz13p1IKnXmrSKaauKB4IkldCkVUxb\nZSBIaqQmrWLaKgNBUiM1aRXTVtmpLKmRmrSKaasMBEmN1ZRVTFtlk5EkCTAQJEkVA0GSBBgIkqSK\ngSBJAgqsdroUETEO/O+0zacBg7L0hcfSfwblOMBj6Ue9Oo4fy8xVCz2pVoEwm4gYbWVZ1zrwWPrP\noBwHeCz9qN+OwyYjSRJgIEiSKoMQCNtLF9BBHkv/GZTjAI+lH/XVcdS+D0GS1BmDcIUgSeqAWgdC\nRGyKiH0R8fWIuLp0PYsVEfdHxJ0RsTciRkvX046IeHdEHIqIu07Y9vSI+GxEfK36/9SSNbZqjmN5\nS0SMVedmb0T8QskaWxERZ0TEv0bEvRFxd0S8vtpeu/Myz7HU8bw8JSL+MyLuqI7lrdX2syLituq8\nfDAiTipWY12bjCJiGfBV4GJgP3A78NrMvKdoYYsQEfcDI3W8lWhEnA88BrwvM8+ptv0V8O3M3FYF\n9amZ+caSdbZijmN5C/BYZv51ydraERHPBp6dmV+JiB8FdgObgauo2XmZ51heTf3OSwBPzczHImIF\n8O/A64E/BHZm5vUR8S7gjsx8Z4ka63yF8ELg65l5X2Z+H7geuLRwTY2TmbcC3562+VLgvdXX72Xi\nF7jvzXEstZOZD2bmV6qvvwPcCwxTw/Myz7HUTk54rHq4ovqXwAXAh6rtRc9LnQNhGHjghMf7qekb\nhYk3xWciYndEbCldTAc8KzMfhIlfaOCZhetZqt+NiP+qmpT6vpnlRBGxFlgP3EbNz8u0Y4EanpeI\nWBYRe4FDwGeB/wEOZ+ax6ilF/47VORBilm31bP+CDZn5s8DPA79TNV2oP7wT+HHgPOBB4G/KltO6\niPgR4MPAGzLz0dL1LMUsx1LL85KZT2TmecAaJlo5fnq2p/W2quPqHAj7gTNOeLwGOFColiXJzAPV\n/4eAjzDxRqmzb1Vtv5NtwIcK17Nomfmt6pf4B8A/UJNzU7VRfxjYkZk7q821PC+zHUtdz8ukzDwM\nfB54EbAyIibvXln071idA+F24CerHvqTgMuBjxWuqW0R8dSqs4yIeCrwCuCu+b+r730MuLL6+krg\nowVrWZLJP6CVV1GDc1N1Xv4TcG9mvu2EXbU7L3MdS03Py6qIWFl9PQRcxESfyL8Cv1w9reh5qe0o\nI4BqqNnbgWXAuzPzLwqX1LaIOJuJqwKYuMf1v9TpOCLiA8DLmVi18VvAm4FdwA3AmcA3gV/JzL7v\nrJ3jWF7ORLNEAvcDvzXZDt+vIuKlwL8BdwI/qDb/KRNt77U6L/Mcy2up33l5PhOdxsuY+DB+Q2b+\nefU34Hrg6cAe4IrM/F6RGuscCJKkzqlzk5EkqYMMBEkSYCBIkioGgiQJMBAkSRUDQZIEGAiSpMry\nhZ8i6UTVktgvAiYXJFsOfHm2bZn5ll7XJy2WgSAtzuXVejRUyxG8YY5tUm3YZCRJAgwESVLFQJAk\nAQaCJKliIEiSAANBklRx2KnUvkPA+yJi8oYtPwTcOMc2qTa8QY4kCbDJSJJUMRAkSYCBIEmqGAiS\nJMBAkCRV/h/gz3ts7T7nNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17ad061e2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制“日期-到达”散点图\n",
    "plt.scatter(local_dailyDelay.day.values.astype('i8'),\n",
    "            local_dailyDelay.avg_arr_delay.values.astype('f8'))\n",
    "\n",
    "# 设置轴标签\n",
    "plt.xlabel('日期')\n",
    "plt.ylabel('到达延误时间')\n",
    "\n",
    "# 绘制x=0水平线\n",
    "plt.axhline(0, color='black', linestyle='--', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （18）关闭SparkSession会话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 关闭SparkSession会话\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
